{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f18dbae",
   "metadata": {},
   "source": [
    "# Sentiment Analysis App with Huggingface Distributed GPU Training and PyTorch Lightning\n",
    "\n",
    "In this notebook, we will reimplement the work done in the SageMaker Project notebook, but with a language model obtained from Huggingface and using PyTorch Lightning to train the model. We are doing this because the model trained in the SageMaker Notebook used an LSTM, which is quite outdated for language models.\n",
    "\n",
    "What you will learn in this notebook:\n",
    "\n",
    "* How to do Distributed GPU Training with a Huggingface model in SageMaker\n",
    "* Tokenizing your text dataset and storing it in s3\n",
    "* Deploying and Testing your trained model\n",
    "* How to use spot instances to train your model\n",
    "\n",
    "References: https://github.com/huggingface/notebooks/blob/master/sagemaker/05_spot_instances/sagemaker-notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6b2a92",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea2f0644",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d37f4ab7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "spacy 3.0.6 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "awscli 1.21.3 requires botocore==1.22.3, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.21.3 requires s3transfer<0.6.0,>=0.5.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.2.4; however, version 21.3.1 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "995bc2af",
   "metadata": {},
   "source": [
    "### Development environment\n",
    "\n",
    "Upgrade ipywidgets for datasets library and restart kernel. Only needed when preprocessing is done in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "964ff8b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.66.2.post0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a671eaa1",
   "metadata": {},
   "source": [
    "### Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01778ae9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using us-east-1.\n",
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "# The name of the bucket doesn't need to contain sagemaker if the IAM role that you use to run the \n",
    "# hyperparameter tuning job has a policy that gives the S3FullAccess permission. \n",
    "BUCKET = 'sagemaker-distributed-hf-imdb-east-1' # The name of the bucket must contain sagemaker, and be globally unique.\n",
    "REGION = boto3.session.Session().region_name\n",
    "# REGION = 'us-east-2' # comment out if you want to use us-east-1\n",
    "\n",
    "try:\n",
    "    if REGION == 'us-east-1':\n",
    "        print('Using us-east-1.')\n",
    "        s3.create_bucket(Bucket=BUCKET)\n",
    "    else: \n",
    "        print(f'Using {REGION}.')\n",
    "        s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={ 'LocationConstraint': REGION })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9686b554",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::034262493329:role/service-role/AmazonSageMaker-ExecutionRole-20210810T151351\n",
      "sagemaker bucket: sagemaker-distributed-hf-imdb-east-1\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=BUCKET\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac2d72",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This section is actually unnecesary if we want to simply train the model, but I've added the steps here for future reference in case we want to use a dataset and task that is not easily provided by Huggingface like IMDB movie classification. In other words, if you only want to train IMDB (or a similarly HF provided dataset), you can simply skip to the training part (Training the Model). For more custom datasets, you will need to do the tokenization before training and may need to modify the training code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a35e2019",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/dataset/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d64f67a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44afb091a664bb1a431267c8fe9dc4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=483.0, style=ProgressStyle(description_…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6377b1aaa37343a6ac3672c8f822f32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34d7fdb1476d435aa1a239772e4b6c27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=466062.0, style=ProgressStyle(descripti…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a0530effe56340f18dce916e5462287f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=28.0, style=ProgressStyle(description_w…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e17b38678144eef931d91e77c9439c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1867.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4496ee250d5747b1a97d4d405978f3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=1004.0, style=ProgressStyle(description…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading and preparing dataset imdb/plain_text (download: 80.23 MiB, generated: 127.06 MiB, post-processed: Unknown size, total: 207.28 MiB) to /home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17640f90ed15426f8dd003967f1c60fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=84125825.0, style=ProgressStyle(descrip…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a01614fb9c3462e8ec970bc0b4742f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf9b3680ccc04810917b207ba2440a55",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44943bedbff749449c8b6c543df4a172",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', layout=Layout(width='20px'), max=1.0), HTML(value=''…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset imdb downloaded and prepared to /home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e0eedfd2dff84b1b8045c49221db9b3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "737994ddd0b040169c7438a311b18a50",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for PyTorch (you can comment the code below once you've run it once)\n",
    "# train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "# train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "# test_dataset = train_dataset.rename_column('label', 'labels')\n",
    "# test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec297444",
   "metadata": {},
   "source": [
    "## Uploading data to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets (with the tokenizer), we are going to use the FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af6df78b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7df619",
   "metadata": {},
   "source": [
    "## Fine-tuning & Starting Training Job\n",
    "\n",
    "At this point we can diverge in a few different directions. \n",
    "\n",
    "We can train the model with 1 GPU for we can do distributed GPU training: we'll be doing distributed GPU training. \n",
    "\n",
    "We can create our own custom `train.py` script to train our model or we can use the premade training scripts provided by huggingface: we'll be using the `run_glue.py` script since it will save us some time from creating our own script. \n",
    "\n",
    "Before we continue, we'll create here the script from the reference notebook in case we want to use the boilerplate `train.py` file created by Huggingface in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa5e948c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_from_disk\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(sys.argv)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval-batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--output_dir\", type=str)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "    test_dataset = load_from_disk(args.test_dir)\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "    # compute metrics function for binary classification\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "    # download model from model hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    if get_last_checkpoint(args.output_dir) is not None:\n",
    "        logger.info(\"***** continue training *****\")\n",
    "        trainer.train(resume_from_checkpoint=args.output_dir)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056f6679",
   "metadata": {},
   "source": [
    "## Creating an Estimator and starting the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58df6bd",
   "metadata": {},
   "source": [
    "If you want to use the `train.py` file or your own custom training file, you need to do the following in the code below:\n",
    "\n",
    "* Remove dataset_name from the hyperparameters\n",
    "* Replace the `entry_point` of `run_glue.py` with `train.py`\n",
    "* Replace the `source_dir` of `./examples/pytorch/text-classification` with `./scripts`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "138df590",
   "metadata": {},
   "source": [
    "### Attach an old training job to an estimator\n",
    "\n",
    "Before we start training, if you have an old trained model you would like to use to continue training, get results, deploy, etc., you can use the following code the grab a model by using the training job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f43259c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name = '' # should be something like huggingface-training-2021-02-04-16-47-39-189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "74a60da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if old_training_job_name != '':\n",
    "    # attach old training job\n",
    "    huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "    # get model output s3 from training job\n",
    "    huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8c542e",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We'll be training our model with Distributed GPU Training. This means that our training job will be parallelized and our model will be trained across several GPUs. In our case, we are using 2 instances of ml.p3.8xlarge which contains 4 GPUs each.\n",
    "\n",
    "Since we are not in a rush to get our training job done, we will be training our model using managed spot instances in order to save money. You can save over 70% when using spot instances. Of course, the downside is that you will have to wait until there are some GPUs available for model training and it is possible you will lose access to your GPUs during training as they are given to those looking for an on-demand GPU. Luckily, we can save checkpoints of our model so even if our spot instance is shut down during training, we will have a checkpoint we can go back to once we start training again. Unfortunately, this is only down automatically if you are using Tensorflow, so you need to pass some parameters to make sure you are checkpointing your model during training. Since we are only training IMDB, I'm not too worried about checkpointing, but I've still provided the necessary code to checkpoint our model.\n",
    "\n",
    "Note: while preparing this notebook, it took a few tries for me to get spot instances for training. Even after waiting for 60 minutes, I did not get spot instances and had to run the training again. It may be because I'm asking for 2 instances of quite powerful ml.p3.8xlarge instances. These are not the most powerful, but I expect that they have much less of these GPUs than the more basic ones (the ones with only one GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "040c4201",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-10-31 17:01:54 Starting - Starting the training job...\n",
      "2021-10-31 17:02:18 Starting - Launching requested ML instancesProfilerReport-1635699708: InProgress\n",
      "......\n",
      "2021-10-31 17:03:18 Starting - Preparing the instances for training.........\n",
      "2021-10-31 17:04:39 Downloading - Downloading input data...\n",
      "2021-10-31 17:05:19 Training - Downloading the training image..............\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2021-10-31 17:07:34,308 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2021-10-31 17:07:34,331 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[35mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[35mbash: no job control in this shell\u001b[0m\n",
      "\u001b[35m2021-10-31 17:07:36,365 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[35m2021-10-31 17:07:36,389 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2021-10-31 17:07:40,558 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2021-10-31 17:07:40,904 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mCollecting accelerate\n",
      "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[34mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[35m2021-10-31 17:07:42,626 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[35m2021-10-31 17:07:42,941 sagemaker-training-toolkit INFO     Installing dependencies from requirements.txt:\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 -m pip install -r requirements.txt\u001b[0m\n",
      "\u001b[34mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[34mSuccessfully installed accelerate-0.5.1\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-31 17:07:43,778 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[34mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"imdb\",\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"do_eval\": true,\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"max_seq_length\": 128,\n",
      "        \"per_device_train_batch_size\": 12,\n",
      "        \"model_name_or_path\": \"bert-base-uncased\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"using-on-demand-2021-10-31-17-01-48-372\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-034262493329/using-on-demand-2021-10-31-17-01-48-372/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\n",
      "\u001b[0m\n",
      "\u001b[34mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"dataset_name\":\"imdb\",\"do_eval\":true,\"do_train\":true,\"fp16\":true,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-uncased\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/checkpoints\",\"pad_to_max_length\":true,\"per_device_train_batch_size\":12}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-034262493329/using-on-demand-2021-10-31-17-01-48-372/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"imdb\",\"do_eval\":true,\"do_train\":true,\"fp16\":true,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-uncased\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/checkpoints\",\"pad_to_max_length\":true,\"per_device_train_batch_size\":12},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"using-on-demand-2021-10-31-17-01-48-372\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-034262493329/using-on-demand-2021-10-31-17-01-48-372/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-1\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--dataset_name\",\"imdb\",\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--fp16\",\"True\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"bert-base-uncased\",\"--num_train_epochs\",\"5\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--pad_to_max_length\",\"True\",\"--per_device_train_batch_size\",\"12\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_DATASET_NAME=imdb\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=bert-base-uncased\u001b[0m\n",
      "\u001b[34mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.6 run_glue.py --dataset_name imdb --do_eval True --do_train True --fp16 True --max_seq_length 128 --model_name_or_path bert-base-uncased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --pad_to_max_length True --per_device_train_batch_size 12\n",
      "\n",
      "\u001b[0m\n",
      "\u001b[35mCollecting accelerate\n",
      "  Downloading accelerate-0.5.1-py3-none-any.whl (58 kB)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: datasets>=1.1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 2)) (1.6.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: sentencepiece!=0.1.92 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 3)) (0.1.91)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: protobuf in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 4)) (3.17.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: torch>=1.3 in /opt/conda/lib/python3.6/site-packages (from -r requirements.txt (line 5)) (1.7.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: fsspec in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2021.5.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: packaging in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (20.9)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dill in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.3.3)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: huggingface-hub<0.1.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.25.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: dataclasses in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.8)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: xxhash in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (2.0.2)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: multiprocess in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (0.70.11.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.19.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: tqdm<4.50.0,>=4.27 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.49.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (4.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pandas in /opt/conda/lib/python3.6/site-packages (from datasets>=1.1.3->-r requirements.txt (line 2)) (1.1.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.6/site-packages (from torch>=1.3->-r requirements.txt (line 5)) (3.10.0.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: filelock in /opt/conda/lib/python3.6/site-packages (from huggingface-hub<0.1.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.12)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: idna<3,>=2.5 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2.10)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (3.0.4)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (2020.12.5)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.6/site-packages (from requests>=2.19.0->datasets>=1.1.3->-r requirements.txt (line 2)) (1.25.11)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyyaml in /opt/conda/lib/python3.6/site-packages (from accelerate->-r requirements.txt (line 1)) (5.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: six>=1.9 in /opt/conda/lib/python3.6/site-packages (from protobuf->-r requirements.txt (line 4)) (1.16.0)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.6/site-packages (from importlib-metadata->datasets>=1.1.3->-r requirements.txt (line 2)) (3.4.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pyparsing>=2.0.2 in /opt/conda/lib/python3.6/site-packages (from packaging->datasets>=1.1.3->-r requirements.txt (line 2)) (2.4.7)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2.8.1)\u001b[0m\n",
      "\u001b[35mRequirement already satisfied: pytz>=2017.2 in /opt/conda/lib/python3.6/site-packages (from pandas->datasets>=1.1.3->-r requirements.txt (line 2)) (2021.1)\u001b[0m\n",
      "\u001b[35mInstalling collected packages: accelerate\u001b[0m\n",
      "\u001b[35mSuccessfully installed accelerate-0.5.1\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as root will break packages and permissions. You should install packages reliably by using venv: https://pip.pypa.io/warnings/venv\n",
      "\u001b[0m\n",
      "\u001b[35m2021-10-31 17:07:45,638 sagemaker-training-toolkit INFO     Invoking user script\n",
      "\u001b[0m\n",
      "\u001b[35mTraining Env:\n",
      "\u001b[0m\n",
      "\u001b[35m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {},\n",
      "    \"current_host\": \"algo-2\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\",\n",
      "        \"algo-2\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"pad_to_max_length\": true,\n",
      "        \"do_train\": true,\n",
      "        \"dataset_name\": \"imdb\",\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"do_eval\": true,\n",
      "        \"output_dir\": \"/opt/ml/checkpoints\",\n",
      "        \"max_seq_length\": 128,\n",
      "        \"per_device_train_batch_size\": 12,\n",
      "        \"model_name_or_path\": \"bert-base-uncased\",\n",
      "        \"fp16\": true\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {},\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": false,\n",
      "    \"job_name\": \"using-on-demand-2021-10-31-17-01-48-372\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-034262493329/using-on-demand-2021-10-31-17-01-48-372/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 8,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-2\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\",\n",
      "            \"algo-2\"\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[35m}\n",
      "\u001b[0m\n",
      "\u001b[35mEnvironment variables:\n",
      "\u001b[0m\n",
      "\u001b[35mSM_HOSTS=[\"algo-1\",\"algo-2\"]\u001b[0m\n",
      "\u001b[35mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[35mSM_HPS={\"dataset_name\":\"imdb\",\"do_eval\":true,\"do_train\":true,\"fp16\":true,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-uncased\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/checkpoints\",\"pad_to_max_length\":true,\"per_device_train_batch_size\":12}\u001b[0m\n",
      "\u001b[35mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[35mSM_RESOURCE_CONFIG={\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[35mSM_INPUT_DATA_CONFIG={}\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[35mSM_CHANNELS=[]\u001b[0m\n",
      "\u001b[35mSM_CURRENT_HOST=algo-2\u001b[0m\n",
      "\u001b[35mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[35mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[35mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[35mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[35mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[35mSM_NUM_CPUS=8\u001b[0m\n",
      "\u001b[35mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[35mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[35mSM_MODULE_DIR=s3://sagemaker-us-east-1-034262493329/using-on-demand-2021-10-31-17-01-48-372/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[35mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{},\"current_host\":\"algo-2\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\",\"algo-2\"],\"hyperparameters\":{\"dataset_name\":\"imdb\",\"do_eval\":true,\"do_train\":true,\"fp16\":true,\"max_seq_length\":128,\"model_name_or_path\":\"bert-base-uncased\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/checkpoints\",\"pad_to_max_length\":true,\"per_device_train_batch_size\":12},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{},\"input_dir\":\"/opt/ml/input\",\"is_master\":false,\"job_name\":\"using-on-demand-2021-10-31-17-01-48-372\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-034262493329/using-on-demand-2021-10-31-17-01-48-372/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":8,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_host\":\"algo-2\",\"hosts\":[\"algo-1\",\"algo-2\"],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[35mSM_USER_ARGS=[\"--dataset_name\",\"imdb\",\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--fp16\",\"True\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"bert-base-uncased\",\"--num_train_epochs\",\"5\",\"--output_dir\",\"/opt/ml/checkpoints\",\"--pad_to_max_length\",\"True\",\"--per_device_train_batch_size\",\"12\"]\u001b[0m\n",
      "\u001b[35mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[35mSM_HP_PAD_TO_MAX_LENGTH=true\u001b[0m\n",
      "\u001b[35mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[35mSM_HP_DATASET_NAME=imdb\u001b[0m\n",
      "\u001b[35mSM_HP_NUM_TRAIN_EPOCHS=5\u001b[0m\n",
      "\u001b[35mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[35mSM_HP_OUTPUT_DIR=/opt/ml/checkpoints\u001b[0m\n",
      "\u001b[35mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[35mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=12\u001b[0m\n",
      "\u001b[35mSM_HP_MODEL_NAME_OR_PATH=bert-base-uncased\u001b[0m\n",
      "\u001b[35mSM_HP_FP16=true\u001b[0m\n",
      "\u001b[35mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python36.zip:/opt/conda/lib/python3.6:/opt/conda/lib/python3.6/lib-dynload:/opt/conda/lib/python3.6/site-packages\n",
      "\u001b[0m\n",
      "\u001b[35mInvoking script with the following command:\n",
      "\u001b[0m\n",
      "\u001b[35m/opt/conda/bin/python3.6 run_glue.py --dataset_name imdb --do_eval True --do_train True --fp16 True --max_seq_length 128 --model_name_or_path bert-base-uncased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --pad_to_max_length True --per_device_train_batch_size 12\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "2021-10-31 17:08:02 Uploading - Uploading generated training model\n",
      "2021-10-31 17:08:02 Failed - Training job failed\n",
      "ProfilerReport-1635699708: Stopping\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run_glue.py\", line 535, in <module>\n",
      "    main()\n",
      "  File \"run_glue.py\", line 188, in main\n",
      "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/hf_argparser.py\", line 187, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 12, in __init__\n",
      "  File \"run_glue.py\", line 131, in __post_init__\n",
      "    raise ValueError(\"Need either a GLUE task or a training/validation file.\")\u001b[0m\n",
      "\u001b[34mValueError: Need either a GLUE task or a training/validation file.\n",
      "\u001b[0m\n",
      "\u001b[34m2021-10-31 17:07:48,909 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.6 run_glue.py --dataset_name imdb --do_eval True --do_train True --fp16 True --max_seq_length 128 --model_name_or_path bert-base-uncased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --pad_to_max_length True --per_device_train_batch_size 12\"\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run_glue.py\", line 535, in <module>\n",
      "    main()\n",
      "  File \"run_glue.py\", line 188, in main\n",
      "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/hf_argparser.py\", line 187, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 12, in __init__\n",
      "  File \"run_glue.py\", line 131, in __post_init__\n",
      "    raise ValueError(\"Need either a GLUE task or a training/validation file.\")\u001b[0m\n",
      "\u001b[34mValueError: Need either a GLUE task or a training/validation file.\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"run_glue.py\", line 535, in <module>\n",
      "    main()\n",
      "  File \"run_glue.py\", line 188, in main\n",
      "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/hf_argparser.py\", line 187, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 12, in __init__\n",
      "  File \"run_glue.py\", line 131, in __post_init__\n",
      "    raise ValueError(\"Need either a GLUE task or a training/validation file.\")\u001b[0m\n",
      "\u001b[35mValueError: Need either a GLUE task or a training/validation file.\n",
      "\u001b[0m\n",
      "\u001b[35m2021-10-31 17:07:49,798 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[35mCommand \"/opt/conda/bin/python3.6 run_glue.py --dataset_name imdb --do_eval True --do_train True --fp16 True --max_seq_length 128 --model_name_or_path bert-base-uncased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --pad_to_max_length True --per_device_train_batch_size 12\"\u001b[0m\n",
      "\u001b[35mTraceback (most recent call last):\n",
      "  File \"run_glue.py\", line 535, in <module>\n",
      "    main()\n",
      "  File \"run_glue.py\", line 188, in main\n",
      "    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n",
      "  File \"/opt/conda/lib/python3.6/site-packages/transformers/hf_argparser.py\", line 187, in parse_args_into_dataclasses\n",
      "    obj = dtype(**inputs)\n",
      "  File \"<string>\", line 12, in __init__\n",
      "  File \"run_glue.py\", line 131, in __post_init__\n",
      "    raise ValueError(\"Need either a GLUE task or a training/validation file.\")\u001b[0m\n",
      "\u001b[35mValueError: Need either a GLUE task or a training/validation file.\u001b[0m\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job using-on-demand-2021-10-31-17-01-48-372: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run_glue.py --dataset_name imdb --do_eval True --do_train True --fp16 True --max_seq_length 128 --model_name_or_path bert-base-uncased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --pad_to_max_length True --per_device_train_batch_size 12\"\nTraceback (most recent call last):\n  File \"run_glue.py\", line 535, in <module>\n    main()\n  File \"run_glue.py\", line 188, in main\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n  File \"/opt/conda/lib/python3.6/site-packages/transformers/hf_argparser.py\", line 187, in parse_args_into_dataclasses\n    obj = dtype(**inputs)\n  File \"<string>\", line 12, in __init__\n  File \"run_glue.py\", line 131, in __post_init__\n    raise ValueError(\"Need either a GLUE task or a training/validation file.\")\nValueError: Need either a GLUE task or a training/validation file.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-b8df0e2bee14>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;31m# starting the train job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# put this inside fit if using train.py: {'train': training_input_path, 'test': test_input_path}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    690\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 692\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    693\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    694\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1650\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1651\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1652\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1653\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1654\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3726\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3727\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3728\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3729\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3282\u001b[0m                 ),\n\u001b[1;32m   3283\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3284\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3285\u001b[0m             )\n\u001b[1;32m   3286\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job using-on-demand-2021-10-31-17-01-48-372: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nCommand \"/opt/conda/bin/python3.6 run_glue.py --dataset_name imdb --do_eval True --do_train True --fp16 True --max_seq_length 128 --model_name_or_path bert-base-uncased --num_train_epochs 5 --output_dir /opt/ml/checkpoints --pad_to_max_length True --per_device_train_batch_size 12\"\nTraceback (most recent call last):\n  File \"run_glue.py\", line 535, in <module>\n    main()\n  File \"run_glue.py\", line 188, in main\n    model_args, data_args, training_args = parser.parse_args_into_dataclasses()\n  File \"/opt/conda/lib/python3.6/site-packages/transformers/hf_argparser.py\", line 187, in parse_args_into_dataclasses\n    obj = dtype(**inputs)\n  File \"<string>\", line 12, in __init__\n  File \"run_glue.py\", line 131, in __post_init__\n    raise ValueError(\"Need either a GLUE task or a training/validation file.\")\nValueError: Need either a GLUE task or a training/validation file."
     ]
    }
   ],
   "source": [
    "# gets role for executing training job\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':'bert-base-uncased',\n",
    "    'output_dir':'/opt/ml/checkpoints', # replace with '/opt/ml/checkpoints' when using train.py\n",
    "    'dataset_name': 'imdb', # remove if using custom training script\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 12,\n",
    "    'num_train_epochs': 5,\n",
    "    'max_seq_length': 128,\n",
    "    'fp16': True,\n",
    "    'pad_to_max_length': True,\n",
    "}\n",
    "\n",
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = 'using-on-demand'\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# smdistributed = SageMaker Distributed\n",
    "distribution = {'smdistributed': {'dataparallel':{'enabled': True}}}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_glue.py', # can be replaced with train.py for custom script\n",
    "    source_dir='./examples/pytorch/text-classification', # can be replaced with local scripts directory ('./scripts')\n",
    "    instance_type='ml.p3.2xlarge', # has 1 GPU each\n",
    "    instance_count=2, # couldn't get 2 spot instances so using 1 for now\n",
    "    base_job_name=job_name,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "#     use_spot_instances=True,\n",
    "#     max_wait=3600, # This should be equal to or greater than max_run in seconds\n",
    "#     max_run=1000, # Expected max run in seconds (so that we don't end up using the instance for too long if there is an issue)\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.6.1',\n",
    "    pytorch_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit() # put this inside fit if using train.py: {'train': training_input_path, 'test': test_input_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76ace1d",
   "metadata": {},
   "source": [
    "## Testing our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1ff5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8492fdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'imdb_sentiment_distributed_transformer'\n",
    "\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "# download model S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f'{local_path}/model.tar.gz', 'r:gz')\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f'{local_path}/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95601ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model=AutoModelForSequenceClassification.from_pretrained(local_path)\n",
    "tokenizer=AutoTokenizer.from_pretrained(local_path)\n",
    "\n",
    "clf = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb6b477",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'The Dark Knight is an excellent film!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb44e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45721c01",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "As usual, to deploy our model we simply need to call `.deploy` on our estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7776030e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "790ad9f5",
   "metadata": {},
   "source": [
    "Let's test out our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e783d0a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = {\"inputs\": \"The Dark Knight is an excellent film!\"}\n",
    "\n",
    "predictor.predict(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c274248b",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf8b472",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904c6ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7499a01",
   "metadata": {},
   "source": [
    "## Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55d0650b",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
