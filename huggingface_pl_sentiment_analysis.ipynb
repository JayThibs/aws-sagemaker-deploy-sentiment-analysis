{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b96d6d50",
   "metadata": {},
   "source": [
    "# Sentiment Analysis App with Huggingface Distributed GPU Training and PyTorch Lightning\n",
    "\n",
    "In this notebook, we will reimplement the work done in the SageMaker Project notebook, but with a language model obtained from Huggingface and using PyTorch Lightning to train the model. We are doing this because the model trained in the SageMaker Notebook used an LSTM, which is quite outdated for language models.\n",
    "\n",
    "What you will learn in this notebook:\n",
    "\n",
    "* How to do Distributed GPU Training with a Huggingface model in SageMaker\n",
    "* Tokenizing your text dataset and storing it in s3\n",
    "* Deploying and Testing your trained model\n",
    "* How to use spot instances to train your model\n",
    "\n",
    "References: https://github.com/huggingface/notebooks/blob/master/sagemaker/05_spot_instances/sagemaker-notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0151570",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e690ce",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "92350224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.53.0.tar.gz (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 26.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 57.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets[s3]==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 69.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 60.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.57.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 69.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (4.0.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.11.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.3)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Collecting botocore==1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 69.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3==1.16.43\n",
      "  Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 68.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 2.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.5)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (20.3.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.15.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.48.0) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from s3fs->datasets[s3]==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.7.4.post0)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Building wheels for collected packages: sagemaker, aiobotocore\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.53.0-py2.py3-none-any.whl size=614386 sha256=4d9a57e18635949b9b6bcbbd1d8b6065c2f0ab8c52556e24998dbc3421457c46\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5b/60/cf/d721091c0164853f9cd36f27e6878ce34454b3f5804e9ce95c\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.2.2-py3-none-any.whl size=45730 sha256=bedfc764880d388ca903551195ca2b13aa21b648942ba6a6645ca856ce43e946\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/37/f3/76/dfc2d32494696a7e4710b2f57d9d15212226d19c42dc395865\n",
      "Successfully built sagemaker aiobotocore\n",
      "Installing collected packages: tqdm, botocore, xxhash, s3transfer, huggingface-hub, aiobotocore, tokenizers, sacremoses, datasets, boto3, transformers, sagemaker\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.57.0\n",
      "    Uninstalling tqdm-4.57.0:\n",
      "      Successfully uninstalled tqdm-4.57.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.100\n",
      "    Uninstalling botocore-1.20.100:\n",
      "      Successfully uninstalled botocore-1.20.100\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.4.2\n",
      "    Uninstalling s3transfer-0.4.2:\n",
      "      Successfully uninstalled s3transfer-0.4.2\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 1.3.0\n",
      "    Uninstalling aiobotocore-1.3.0:\n",
      "      Successfully uninstalled aiobotocore-1.3.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.100\n",
      "    Uninstalling boto3-1.17.100:\n",
      "      Successfully uninstalled boto3-1.17.100\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.45.0\n",
      "    Uninstalling sagemaker-2.45.0:\n",
      "      Successfully uninstalled sagemaker-2.45.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "spacy 3.0.3 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "awscli 1.19.100 requires botocore==1.20.100, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.19.100 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed aiobotocore-1.2.2 boto3-1.16.43 botocore-1.19.52 datasets-1.6.2 huggingface-hub-0.0.8 s3transfer-0.3.7 sacremoses-0.0.45 sagemaker-2.53.0 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.6.1 xxhash-2.0.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8735c59",
   "metadata": {},
   "source": [
    "### Development environment\n",
    "\n",
    "Upgrade ipywidgets for datasets library and restart kernel. Only needed when preprocessing is done in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "423967d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1589b43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.53.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff6ac7b",
   "metadata": {},
   "source": [
    "### Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35f87873",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::034262493329:role/service-role/AmazonSageMaker-ExecutionRole-20210810T151351\n",
      "sagemaker bucket: sagemaker-us-east-1-034262493329\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6c06f5",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This section is actually unnecesary if we want to simply train the model, but I've added the steps here for future reference in case we want to use a dataset and task that is not easily provided by Huggingface like IMDB movie classification. In other words, if you only want to train IMDB (or a similarly HF provided dataset), you can simply skip to the training part. For more custom datasets, you will need to do the tokenization before training and may need to modify the training code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a32f0537",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/dataset/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c60603a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822cc6368cb04c7585ccc08c74705b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df16be51caa344b99a29582e08f86bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for PyTorch\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = train_dataset.rename_column('label', 'labels')\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0443c8b",
   "metadata": {},
   "source": [
    "## Uploading data to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets (with the tokenizer), we are going to use the FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2fdf8463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd3b4ba",
   "metadata": {},
   "source": [
    "## Fine-tuning & Starting Training Job\n",
    "\n",
    "At this point we can diverge in a few different directions. \n",
    "\n",
    "We can train the model with 1 GPU for we can do distributed GPU training: we'll be doing distributed GPU training. \n",
    "\n",
    "We can create our own custom `train.py` script to train our model or we can use the premade training scripts provided by huggingface: we'll be using the `run_glue.py` script since it will save us some time from creating our own script. \n",
    "\n",
    "Before we continue, we'll create here the script from the reference notebook in case we want to use the boilerplate `train.py` file created by Huggingface in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8adfe8a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_from_disk\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(sys.argv)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval-batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--output_dir\", type=str)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "    test_dataset = load_from_disk(args.test_dir)\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "    # compute metrics function for binary classification\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "    # download model from model hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    if get_last_checkpoint(args.output_dir) is not None:\n",
    "        logger.info(\"***** continue training *****\")\n",
    "        trainer.train(resume_from_checkpoint=args.output_dir)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca0907f",
   "metadata": {},
   "source": [
    "## Creating an Estimator and starting the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0962c364",
   "metadata": {},
   "source": [
    "If you want to use the `train.py` file or your own custom training file, you need to do the following in the code below:\n",
    "\n",
    "* Remove dataset_name from the hyperparameters\n",
    "* Replace the `entry_point` of `run_glue.py` with `train.py`\n",
    "* Replace the `source_dir` of `./examples/pytorch/text-classification` with `./scripts`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db9d4bd8",
   "metadata": {},
   "source": [
    "### Attach an old training job to an estimator\n",
    "\n",
    "Before we start training, if you have an old trained model you would like to use to continue training, get results, deploy, etc., you can use the following code the grab a model by using the training job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948571d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name = '' # should be something like huggingface-training-2021-02-04-16-47-39-189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c129dfa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3628773",
   "metadata": {},
   "source": [
    "### Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715cf4b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-12 20:11:54 Starting - Starting the training job...\n",
      "2021-08-12 20:11:56 Starting - Launching requested ML instancesProfilerReport-1628799107: InProgress\n",
      "...\n",
      "2021-08-12 20:12:53 Starting - Insufficient capacity error from EC2 while launching instances, retrying!.............................................................................................................................................ProfilerReport-1628799107: Stopped\n",
      "........................................................................................................................................................................................................................"
     ]
    }
   ],
   "source": [
    "# gets role for executing training job\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':'distilbert-base-uncased',\n",
    "    'output_dir':'/opt/ml/checkpoints', # replace with '/opt/ml/checkpoints' when using train.py\n",
    "    'dataset_name': 'imdb', # remove if using custom training script\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 12,\n",
    "    'num_train_epochs': 5,\n",
    "    'max_seq_length': 128,\n",
    "    'fp16': True,\n",
    "    'pad_to_max_length': True,\n",
    "}\n",
    "\n",
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = 'using-spot'\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# smdistributed = SageMaker Distributed\n",
    "distribution = {'smdistributed': {'dataparallel':{'enabled': True}}}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_glue.py', # can be replaced with train.py for custom script\n",
    "    source_dir='./examples/pytorch/text-classification', # can be replaced with local scripts directory ('./scripts')\n",
    "    instance_type='ml.p3.8xlarge', # has 4 GPUs\n",
    "    instance_count=2, # changed to 2 instances\n",
    "    base_job_name=job_name,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600, # This should be equal to or greater than max_run in seconds\n",
    "    max_run=1000, # Expected max run in seconds (so that we don't end up using the instance for too long if there is an issue)\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.6.1',\n",
    "    pytorch_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit() # put this inside fit if using train.py: {'train': training_input_path, 'test': test_input_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1431c064",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a65623",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'imdb_sentiment_distributed_transformer'\n",
    "\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "# download model S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f'{local_path}/model.tar.gz', 'r:gz')\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f'{local_path}/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e94731d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model=AutoModelForSequenceClassification.from_pretrained(local_path)\n",
    "tokenizer=AutoTokenizer.from_pretrained(local_path)\n",
    "\n",
    "clf = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b49641",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'The Dark Knight is an excellent film!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e92911",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3729150b",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "As usual, to deploy our model we simply need to call `.deploy` on our estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5654e2ef",
   "metadata": {},
   "source": [
    "Let's test out our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdb242f",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = {\"inputs\": \"The Dark Knight is an excellent film!\"}\n",
    "\n",
    "predictor.predict(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b9d238",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e56e339",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef73cebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e20104",
   "metadata": {},
   "source": [
    "## Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa36a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
