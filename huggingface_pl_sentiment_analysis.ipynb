{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21be4bbc",
   "metadata": {},
   "source": [
    "# Sentiment Analysis App with Huggingface Distributed GPU Training and PyTorch Lightning\n",
    "\n",
    "In this notebook, we will reimplement the work done in the SageMaker Project notebook, but with a language model obtained from Huggingface and using PyTorch Lightning to train the model. We are doing this because the model trained in the SageMaker Notebook used an LSTM, which is quite outdated for language models.\n",
    "\n",
    "References: https://github.com/huggingface/notebooks/blob/master/sagemaker/05_spot_instances/sagemaker-notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "622aec41",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6784c30f",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d98fac38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sagemaker>=2.48.0\n",
      "  Downloading sagemaker-2.53.0.tar.gz (438 kB)\n",
      "\u001b[K     |████████████████████████████████| 438 kB 26.8 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting transformers==4.6.1\n",
      "  Downloading transformers-4.6.1-py3-none-any.whl (2.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 2.2 MB 57.1 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting datasets[s3]==1.6.2\n",
      "  Downloading datasets-1.6.2-py3-none-any.whl (221 kB)\n",
      "\u001b[K     |████████████████████████████████| 221 kB 69.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting huggingface-hub==0.0.8\n",
      "  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n",
      "Collecting tokenizers<0.11,>=0.10.1\n",
      "  Downloading tokenizers-0.10.3-cp36-cp36m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n",
      "\u001b[K     |████████████████████████████████| 3.3 MB 60.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2.25.1)\n",
      "Requirement already satisfied: dataclasses in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (0.8)\n",
      "Requirement already satisfied: importlib-metadata in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (1.19.5)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (3.0.12)\n",
      "Collecting sacremoses\n",
      "  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n",
      "\u001b[K     |████████████████████████████████| 895 kB 69.1 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (2020.11.13)\n",
      "Requirement already satisfied: packaging in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (20.9)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from transformers==4.6.1) (4.57.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp36-cp36m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[K     |████████████████████████████████| 243 kB 69.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: pyarrow>=1.0.0<4.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (4.0.1)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.70.11.1)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (1.1.5)\n",
      "Requirement already satisfied: dill in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (0.3.3)\n",
      "Collecting tqdm>=4.27\n",
      "  Downloading tqdm-4.49.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[K     |████████████████████████████████| 69 kB 11.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: fsspec in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Collecting botocore==1.19.52\n",
      "  Downloading botocore-1.19.52-py2.py3-none-any.whl (7.2 MB)\n",
      "\u001b[K     |████████████████████████████████| 7.2 MB 69.6 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting boto3==1.16.43\n",
      "  Downloading boto3-1.16.43-py2.py3-none-any.whl (130 kB)\n",
      "\u001b[K     |████████████████████████████████| 130 kB 68.8 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: s3fs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from datasets[s3]==1.6.2) (2021.4.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from boto3==1.16.43->datasets[s3]==1.6.2) (0.10.0)\n",
      "Collecting s3transfer<0.4.0,>=0.3.0\n",
      "  Downloading s3transfer-0.3.7-py2.py3-none-any.whl (73 kB)\n",
      "\u001b[K     |████████████████████████████████| 73 kB 2.7 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (2.8.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.25.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from botocore==1.19.52->datasets[s3]==1.6.2) (1.26.5)\n",
      "Requirement already satisfied: attrs in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (20.3.0)\n",
      "Requirement already satisfied: google-pasta in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (3.15.2)\n",
      "Requirement already satisfied: protobuf3-to-dict>=0.1.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.1.5)\n",
      "Requirement already satisfied: smdebug_rulesconfig==1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (1.0.1)\n",
      "Requirement already satisfied: pathos in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sagemaker>=2.48.0) (0.2.7)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from importlib-metadata->transformers==4.6.1) (3.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from packaging->transformers==4.6.1) (2.4.7)\n",
      "Requirement already satisfied: six>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from protobuf>=3.1->sagemaker>=2.48.0) (1.15.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from requests->transformers==4.6.1) (2021.5.30)\n",
      "Requirement already satisfied: pytz>=2017.2 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pandas->datasets[s3]==1.6.2) (2021.1)\n",
      "Requirement already satisfied: ppft>=1.6.6.3 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (1.6.6.3)\n",
      "Requirement already satisfied: pox>=0.2.9 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from pathos->sagemaker>=2.48.0) (0.2.9)\n",
      "Requirement already satisfied: aiobotocore>=1.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from s3fs->datasets[s3]==1.6.2) (1.3.0)\n",
      "Requirement already satisfied: aiohttp>=3.3.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.7.4.post0)\n",
      "Requirement already satisfied: aioitertools>=0.5.1 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (0.7.1)\n",
      "Requirement already satisfied: wrapt>=1.10.10 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.12.1)\n",
      "Collecting aiobotocore>=1.0.1\n",
      "  Downloading aiobotocore-1.3.3.tar.gz (50 kB)\n",
      "\u001b[K     |████████████████████████████████| 50 kB 10.3 MB/s eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.2.tar.gz (49 kB)\n",
      "\u001b[K     |████████████████████████████████| 49 kB 1.3 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.3.1.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 1.4 MB/s  eta 0:00:01\n",
      "\u001b[?25h  Downloading aiobotocore-1.2.2.tar.gz (48 kB)\n",
      "\u001b[K     |████████████████████████████████| 48 kB 7.0 MB/s  eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: idna-ssl>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (1.6.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (5.1.0)\n",
      "Requirement already satisfied: async-timeout<4.0,>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from aiohttp>=3.3.1->aiobotocore>=1.0.1->s3fs->datasets[s3]==1.6.2) (3.0.1)\n",
      "Requirement already satisfied: joblib in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (1.0.1)\n",
      "Requirement already satisfied: click in /home/ec2-user/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages (from sacremoses->transformers==4.6.1) (7.1.2)\n",
      "Building wheels for collected packages: sagemaker, aiobotocore\n",
      "  Building wheel for sagemaker (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for sagemaker: filename=sagemaker-2.53.0-py2.py3-none-any.whl size=614386 sha256=4d9a57e18635949b9b6bcbbd1d8b6065c2f0ab8c52556e24998dbc3421457c46\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/5b/60/cf/d721091c0164853f9cd36f27e6878ce34454b3f5804e9ce95c\n",
      "  Building wheel for aiobotocore (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for aiobotocore: filename=aiobotocore-1.2.2-py3-none-any.whl size=45730 sha256=bedfc764880d388ca903551195ca2b13aa21b648942ba6a6645ca856ce43e946\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/37/f3/76/dfc2d32494696a7e4710b2f57d9d15212226d19c42dc395865\n",
      "Successfully built sagemaker aiobotocore\n",
      "Installing collected packages: tqdm, botocore, xxhash, s3transfer, huggingface-hub, aiobotocore, tokenizers, sacremoses, datasets, boto3, transformers, sagemaker\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.57.0\n",
      "    Uninstalling tqdm-4.57.0:\n",
      "      Successfully uninstalled tqdm-4.57.0\n",
      "  Attempting uninstall: botocore\n",
      "    Found existing installation: botocore 1.20.100\n",
      "    Uninstalling botocore-1.20.100:\n",
      "      Successfully uninstalled botocore-1.20.100\n",
      "  Attempting uninstall: s3transfer\n",
      "    Found existing installation: s3transfer 0.4.2\n",
      "    Uninstalling s3transfer-0.4.2:\n",
      "      Successfully uninstalled s3transfer-0.4.2\n",
      "  Attempting uninstall: aiobotocore\n",
      "    Found existing installation: aiobotocore 1.3.0\n",
      "    Uninstalling aiobotocore-1.3.0:\n",
      "      Successfully uninstalled aiobotocore-1.3.0\n",
      "  Attempting uninstall: boto3\n",
      "    Found existing installation: boto3 1.17.100\n",
      "    Uninstalling boto3-1.17.100:\n",
      "      Successfully uninstalled boto3-1.17.100\n",
      "  Attempting uninstall: sagemaker\n",
      "    Found existing installation: sagemaker 2.45.0\n",
      "    Uninstalling sagemaker-2.45.0:\n",
      "      Successfully uninstalled sagemaker-2.45.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "spacy 3.0.3 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "awscli 1.19.100 requires botocore==1.20.100, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.19.100 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "Successfully installed aiobotocore-1.2.2 boto3-1.16.43 botocore-1.19.52 datasets-1.6.2 huggingface-hub-0.0.8 s3transfer-0.3.7 sacremoses-0.0.45 sagemaker-2.53.0 tokenizers-0.10.3 tqdm-4.49.0 transformers-4.6.1 xxhash-2.0.2\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fa66fa",
   "metadata": {},
   "source": [
    "### Development environment\n",
    "\n",
    "Upgrade ipywidgets for datasets library and restart kernel. Only needed when preprocessing is done in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4ce6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "755a508c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.53.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beed3035",
   "metadata": {},
   "source": [
    "### Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af7b9ded",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::034262493329:role/service-role/AmazonSageMaker-ExecutionRole-20210810T151351\n",
      "sagemaker bucket: sagemaker-us-east-1-034262493329\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=None\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9265fb20",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This section is actually unnecesary if we want to simply train the model, but I've added the steps here for future reference in case we want to use a dataset and task that is not easily provided by Huggingface like IMDB movie classification. In other words, if you only want to train IMDB (or a similarly HF provided dataset), you can simply skip to the training part. For more custom datasets, you will need to do the tokenization before training and may need to modify the training code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f802eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/dataset/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "837c6fdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "822cc6368cb04c7585ccc08c74705b2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df16be51caa344b99a29582e08f86bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for PyTorch\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = train_dataset.rename_column('label', 'labels')\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f06fcb",
   "metadata": {},
   "source": [
    "## Uploading data to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets (with the tokenizer), we are going to use the FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "89f4930d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8944901c",
   "metadata": {},
   "source": [
    "## Fine-tuning & Starting Training Job\n",
    "\n",
    "At this point we can diverge in a few different directions. \n",
    "\n",
    "We can train the model with 1 GPU for we can do distributed GPU training: we'll be doing distributed GPU training. \n",
    "\n",
    "We can create our own custom `train.py` script to train our model or we can use the premade training scripts provided by huggingface: we'll be using the `run_glue.py` script since it will save us some time from creating our own script. \n",
    "\n",
    "Before we continue, we'll create here the script from the reference notebook in case we want to use the boilerplate `train.py` file created by Huggingface in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5da723a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_from_disk\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(sys.argv)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval-batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--output_dir\", type=str)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "    test_dataset = load_from_disk(args.test_dir)\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "    # compute metrics function for binary classification\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "    # download model from model hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    if get_last_checkpoint(args.output_dir) is not None:\n",
    "        logger.info(\"***** continue training *****\")\n",
    "        trainer.train(resume_from_checkpoint=args.output_dir)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af3d62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41899424",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4136fae",
   "metadata": {},
   "outputs": [
    {
     "ename": "ResourceLimitExceeded",
     "evalue": "An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.8xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-153443bbede5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;31m# starting the train job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    681\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_for_training\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    682\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 683\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_TrainingJob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart_new\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    684\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    685\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mstart_new\u001b[0;34m(cls, estimator, inputs, experiment_config)\u001b[0m\n\u001b[1;32m   1453\u001b[0m         \"\"\"\n\u001b[1;32m   1454\u001b[0m         \u001b[0mtrain_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_train_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexperiment_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1455\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1457\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_current_job_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, input_mode, input_config, role, job_name, output_config, resource_config, vpc_config, hyperparameters, stop_condition, tags, metric_definitions, enable_network_isolation, image_uri, algorithm_arn, encrypt_inter_container_traffic, use_spot_instances, checkpoint_s3_uri, checkpoint_local_path, experiment_config, debugger_rule_configs, debugger_hook_config, tensorboard_output_config, enable_sagemaker_metrics, profiler_rule_configs, profiler_config, environment, retry_strategy)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Creating training-job with name: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m         \u001b[0mLOGGER\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train request: %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdumps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_training_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_request\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     def _get_train_request(  # noqa: C901\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    384\u001b[0m                     \"%s() only accepts keyword arguments.\" % py_operation_name)\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# The \"self\" in this scope is referring to the BaseClient.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_api_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moperation_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    387\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0m_api_call\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_operation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m_make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0merror_code\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Code\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0merror_class\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_code\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_code\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0merror_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_response\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moperation_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mparsed_response\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceLimitExceeded\u001b[0m: An error occurred (ResourceLimitExceeded) when calling the CreateTrainingJob operation: The account-level service limit 'ml.p3.8xlarge for training job usage' is 0 Instances, with current utilization of 0 Instances and a request delta of 2 Instances. Please contact AWS support to request an increase for this limit."
     ]
    }
   ],
   "source": [
    "# gets role for executing training job\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':'distilbert-base-uncased',\n",
    "    'output_dir':'/opt/ml/model',\n",
    "    'dataset_name': 'imdb',\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 12,\n",
    "    'num_train_epochs': 5,\n",
    "    'max_seq_length': 128,\n",
    "    'fp16': True,\n",
    "    'pad_to_max_length': True,\n",
    "}\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# smdistributed = SageMaker Distributed\n",
    "distribution = {'smdistributed': {'dataparallel':{'enabled': True}}}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_glue.py',\n",
    "    source_dir='./examples/pytorch/text-classification',\n",
    "    instance_type='ml.p3.8xlarge', # has 4 GPUs\n",
    "    instance_count=2, # changed to 2 instances\n",
    "    base_job_name=job_name,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600, # This should be equal to or greater than max_run in seconds\n",
    "    max_run=1000, # Expected max run in seconds (so that we don't end up using the instance for too long if there is an issue)\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.6.1',\n",
    "    pytorch_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da03a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5bc2a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'imdb_sentiment_distributed_transformer'\n",
    "\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "# download model S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f'{local_path}/model.tar.gz', 'r:gz')\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f'{local_path}/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1517a5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model=AutoModelForSequenceClassification.from_pretrained(local_path)\n",
    "tokenizer=AutoTokenizer.from_pretrained(local_path)\n",
    "\n",
    "clf = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815058a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263e9873",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
