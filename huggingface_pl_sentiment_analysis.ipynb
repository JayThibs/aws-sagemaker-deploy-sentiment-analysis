{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d7641d98",
   "metadata": {},
   "source": [
    "# Sentiment Analysis App with Huggingface Distributed GPU Training and PyTorch Lightning\n",
    "\n",
    "In this notebook, we will reimplement the work done in the SageMaker Project notebook, but with a language model obtained from Huggingface and using PyTorch Lightning to train the model. We are doing this because the model trained in the SageMaker Notebook used an LSTM, which is quite outdated for language models.\n",
    "\n",
    "What you will learn in this notebook:\n",
    "\n",
    "* How to do Distributed GPU Training with a Huggingface model in SageMaker\n",
    "* Tokenizing your text dataset and storing it in s3\n",
    "* Deploying and Testing your trained model\n",
    "* How to use spot instances to train your model\n",
    "\n",
    "References: https://github.com/huggingface/notebooks/blob/master/sagemaker/05_spot_instances/sagemaker-notebook.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441f2ca6",
   "metadata": {},
   "source": [
    "## Development Environment and Permissions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed7e931",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "febe42c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 1.0.61 requires nvidia-ml-py3, which is not installed.\n",
      "spacy 3.0.3 requires pydantic<1.8.0,>=1.7.1, but you have pydantic 1.8.2 which is incompatible.\n",
      "awscli 1.19.100 requires botocore==1.20.100, but you have botocore 1.19.52 which is incompatible.\n",
      "awscli 1.19.100 requires s3transfer<0.5.0,>=0.4.0, but you have s3transfer 0.3.7 which is incompatible.\u001b[0m\n",
      "\u001b[33mWARNING: You are using pip version 21.1.2; however, version 21.2.4 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/pytorch_p36/bin/python -m pip install --upgrade pip' command.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install \"sagemaker>=2.48.0\" \"transformers==4.6.1\" \"datasets[s3]==1.6.2\" --upgrade --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0974daf5",
   "metadata": {},
   "source": [
    "### Development environment\n",
    "\n",
    "Upgrade ipywidgets for datasets library and restart kernel. Only needed when preprocessing is done in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98a3d56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "!conda install -c conda-forge ipywidgets -y\n",
    "IPython.Application.instance().kernel.do_shutdown(True) # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b20a43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.53.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "sagemaker.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77ec838",
   "metadata": {},
   "source": [
    "### Permissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de989e65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using us-east-2.\n",
      "S3 bucket created successfully\n"
     ]
    }
   ],
   "source": [
    "import boto3\n",
    "s3 = boto3.resource('s3')\n",
    "# The name of the bucket doesn't need to contain sagemaker if the IAM role that you use to run the \n",
    "# hyperparameter tuning job has a policy that gives the S3FullAccess permission. \n",
    "BUCKET = 'sagemaker-distributed-hf-imdb-east-2' # The name of the bucket must contain sagemaker, and be globally unique.\n",
    "REGION = boto3.session.Session().region_name\n",
    "REGION = 'us-east-2' # comment out if you want to use us-east-1\n",
    "\n",
    "try:\n",
    "    if REGION == 'us-east-1':\n",
    "        print('Using us-east-1.')\n",
    "        s3.create_bucket(Bucket=BUCKET)\n",
    "    else: \n",
    "        print(f'Using {REGION}.')\n",
    "        s3.create_bucket(Bucket=BUCKET, CreateBucketConfiguration={ 'LocationConstraint': REGION })\n",
    "    print('S3 bucket created successfully')\n",
    "except Exception as e:\n",
    "    print('S3 error: ',e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "46b7c691",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'boto_region_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-f08ce5021b99>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdefault_bucket\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msagemaker_session_bucket\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mboto_region_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mREGION\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'boto_region_name'"
     ]
    }
   ],
   "source": [
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket, boto_region_name=REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e60c3a2b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'SageMakerRuntime' object has no attribute 'boto_region_name'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-958fbad67a2e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboto_region_name\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/botocore/client.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m    643\u001b[0m         raise AttributeError(\n\u001b[1;32m    644\u001b[0m             \"'%s' object has no attribute '%s'\" % (\n\u001b[0;32m--> 645\u001b[0;31m                 self.__class__.__name__, item)\n\u001b[0m\u001b[1;32m    646\u001b[0m         )\n\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SageMakerRuntime' object has no attribute 'boto_region_name'"
     ]
    }
   ],
   "source": [
    "sess.boto_region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3e109eaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker role arn: arn:aws:iam::034262493329:role/service-role/AmazonSageMaker-ExecutionRole-20210810T151351\n",
      "sagemaker bucket: sagemaker-distributed-hf-imdb-east-2\n",
      "sagemaker session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "# sess = sagemaker.Session()\n",
    "# sagemaker session bucket -> used for uploading data, models and logs\n",
    "# sagemaker will automatically create this bucket if it not exists\n",
    "sagemaker_session_bucket=BUCKET\n",
    "if sagemaker_session_bucket is None and sess is not None:\n",
    "    # set to default bucket if a bucket name is not given\n",
    "    sagemaker_session_bucket = sess.default_bucket()\n",
    "\n",
    "role = sagemaker.get_execution_role()\n",
    "sess = sagemaker.Session(default_bucket=sagemaker_session_bucket)\n",
    "\n",
    "print(f\"sagemaker role arn: {role}\")\n",
    "print(f\"sagemaker bucket: {sess.default_bucket()}\")\n",
    "print(f\"sagemaker session region: {sess.boto_region_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa65dae",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "\n",
    "This section is actually unnecesary if we want to simply train the model, but I've added the steps here for future reference in case we want to use a dataset and task that is not easily provided by Huggingface like IMDB movie classification. In other words, if you only want to train IMDB (or a similarly HF provided dataset), you can simply skip to the training part (Training the Model). For more custom datasets, you will need to do the tokenization before training and may need to modify the training code a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d7fd1ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# tokenizer used in preprocessing\n",
    "tokenizer_name = 'distilbert-base-uncased'\n",
    "\n",
    "# dataset used\n",
    "dataset_name = 'imdb'\n",
    "\n",
    "# s3 key prefix for the data\n",
    "s3_prefix = 'samples/dataset/imdb'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d84cc56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset imdb (/home/ec2-user/.cache/huggingface/datasets/imdb/plain_text/1.0.0/4ea52f2e58a08dbc12c2bd52d0d92b30b88c00230b4522801b3636782f625c5b)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2cfca7fffebc489fb9ea509b54807bbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=25.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54983f6d9c6403d91e0d70d62d3a4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# download tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(tokenizer_name)\n",
    "\n",
    "# tokenizer helper function\n",
    "def tokenize(batch):\n",
    "    return tokenizer(batch['text'], padding='max_length', truncation=True)\n",
    "\n",
    "# load dataset\n",
    "train_dataset, test_dataset = load_dataset('imdb', split=['train', 'test'])\n",
    "test_dataset = test_dataset.shuffle().select(range(10000))\n",
    "\n",
    "# tokenize dataset\n",
    "train_dataset = train_dataset.map(tokenize, batched=True)\n",
    "test_dataset = test_dataset.map(tokenize, batched=True)\n",
    "\n",
    "# set format for PyTorch (you can comment the code below once you've run it once)\n",
    "train_dataset = train_dataset.rename_column('label', 'labels')\n",
    "train_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])\n",
    "test_dataset = train_dataset.rename_column('label', 'labels')\n",
    "test_dataset.set_format('torch', columns=['input_ids', 'attention_mask', 'labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a78fe9b",
   "metadata": {},
   "source": [
    "## Uploading data to sagemaker_session_bucket\n",
    "\n",
    "After we processed the datasets (with the tokenizer), we are going to use the FileSystem integration to upload our dataset to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2abb5da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "\n",
    "# save train_dataset to s3\n",
    "training_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/train'\n",
    "train_dataset.save_to_disk(training_input_path, fs=s3)\n",
    "\n",
    "# save test_dataset to s3\n",
    "test_input_path = f's3://{sess.default_bucket()}/{s3_prefix}/test'\n",
    "test_dataset.save_to_disk(test_input_path, fs=s3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b644c2",
   "metadata": {},
   "source": [
    "## Fine-tuning & Starting Training Job\n",
    "\n",
    "At this point we can diverge in a few different directions. \n",
    "\n",
    "We can train the model with 1 GPU for we can do distributed GPU training: we'll be doing distributed GPU training. \n",
    "\n",
    "We can create our own custom `train.py` script to train our model or we can use the premade training scripts provided by huggingface: we'll be using the `run_glue.py` script since it will save us some time from creating our own script. \n",
    "\n",
    "Before we continue, we'll create here the script from the reference notebook in case we want to use the boilerplate `train.py` file created by Huggingface in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "06ad1258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./scripts/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ./scripts/train.py\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from datasets import load_from_disk\n",
    "import logging\n",
    "import sys\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "# Set up logging\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.getLevelName(\"INFO\"),\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    ")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    logger.info(sys.argv)\n",
    "\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # hyperparameters sent by the client are passed as command-line arguments to the script.\n",
    "    parser.add_argument(\"--epochs\", type=int, default=3)\n",
    "    parser.add_argument(\"--train-batch-size\", type=int, default=32)\n",
    "    parser.add_argument(\"--eval-batch-size\", type=int, default=64)\n",
    "    parser.add_argument(\"--warmup_steps\", type=int, default=500)\n",
    "    parser.add_argument(\"--model_name\", type=str)\n",
    "    parser.add_argument(\"--learning_rate\", type=str, default=5e-5)\n",
    "    parser.add_argument(\"--output_dir\", type=str)\n",
    "\n",
    "    # Data, model, and output directories\n",
    "    parser.add_argument(\"--output-data-dir\", type=str, default=os.environ[\"SM_OUTPUT_DATA_DIR\"])\n",
    "    parser.add_argument(\"--model-dir\", type=str, default=os.environ[\"SM_MODEL_DIR\"])\n",
    "    parser.add_argument(\"--n_gpus\", type=str, default=os.environ[\"SM_NUM_GPUS\"])\n",
    "    parser.add_argument(\"--training_dir\", type=str, default=os.environ[\"SM_CHANNEL_TRAIN\"])\n",
    "    parser.add_argument(\"--test_dir\", type=str, default=os.environ[\"SM_CHANNEL_TEST\"])\n",
    "\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # load datasets\n",
    "    train_dataset = load_from_disk(args.training_dir)\n",
    "    test_dataset = load_from_disk(args.test_dir)\n",
    "\n",
    "    logger.info(f\" loaded train_dataset length is: {len(train_dataset)}\")\n",
    "    logger.info(f\" loaded test_dataset length is: {len(test_dataset)}\")\n",
    "\n",
    "    # compute metrics function for binary classification\n",
    "    def compute_metrics(pred):\n",
    "        labels = pred.label_ids\n",
    "        preds = pred.predictions.argmax(-1)\n",
    "        precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average=\"binary\")\n",
    "        acc = accuracy_score(labels, preds)\n",
    "        return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n",
    "\n",
    "    # download model from model hub\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(args.model_name)\n",
    "\n",
    "    # define training args\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=args.output_dir,\n",
    "        num_train_epochs=args.epochs,\n",
    "        per_device_train_batch_size=args.train_batch_size,\n",
    "        per_device_eval_batch_size=args.eval_batch_size,\n",
    "        warmup_steps=args.warmup_steps,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        logging_dir=f\"{args.output_data_dir}/logs\",\n",
    "        learning_rate=float(args.learning_rate),\n",
    "    )\n",
    "\n",
    "    # create Trainer instance\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        compute_metrics=compute_metrics,\n",
    "        train_dataset=train_dataset,\n",
    "        eval_dataset=test_dataset,\n",
    "    )\n",
    "\n",
    "    # train model\n",
    "    if get_last_checkpoint(args.output_dir) is not None:\n",
    "        logger.info(\"***** continue training *****\")\n",
    "        trainer.train(resume_from_checkpoint=args.output_dir)\n",
    "    else:\n",
    "        trainer.train()\n",
    "    # evaluate model\n",
    "    eval_result = trainer.evaluate(eval_dataset=test_dataset)\n",
    "\n",
    "    # writes eval result to file which can be accessed later in s3 ouput\n",
    "    with open(os.path.join(args.output_data_dir, \"eval_results.txt\"), \"w\") as writer:\n",
    "        print(f\"***** Eval results *****\")\n",
    "        for key, value in sorted(eval_result.items()):\n",
    "            writer.write(f\"{key} = {value}\\n\")\n",
    "\n",
    "    # Saves the model to s3\n",
    "    trainer.save_model(args.model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86386b11",
   "metadata": {},
   "source": [
    "## Creating an Estimator and starting the training job"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443e1700",
   "metadata": {},
   "source": [
    "If you want to use the `train.py` file or your own custom training file, you need to do the following in the code below:\n",
    "\n",
    "* Remove dataset_name from the hyperparameters\n",
    "* Replace the `entry_point` of `run_glue.py` with `train.py`\n",
    "* Replace the `source_dir` of `./examples/pytorch/text-classification` with `./scripts`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46904a1",
   "metadata": {},
   "source": [
    "### Attach an old training job to an estimator\n",
    "\n",
    "Before we start training, if you have an old trained model you would like to use to continue training, get results, deploy, etc., you can use the following code the grab a model by using the training job name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfea6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# job which is going to be attached to the estimator\n",
    "old_training_job_name = '' # should be something like huggingface-training-2021-02-04-16-47-39-189"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9643119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# attach old training job\n",
    "huggingface_estimator_loaded = Estimator.attach(old_training_job_name)\n",
    "\n",
    "# get model output s3 from training job\n",
    "huggingface_estimator_loaded.model_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce6e668",
   "metadata": {},
   "source": [
    "### Training the Model\n",
    "\n",
    "We'll be training our model with Distributed GPU Training. This means that our training job will be parallelized and our model will be trained across several GPUs. In our case, we are using 2 instances of ml.p3.8xlarge which contains 4 GPUs each.\n",
    "\n",
    "Since we are not in a rush to get our training job done, we will be training our model using managed spot instances in order to save money. You can save over 70% when using spot instances. Of course, the downside is that you will have to wait until there are some GPUs available for model training and it is possible you will lose access to your GPUs during training as they are given to those looking for an on-demand GPU. Luckily, we can save checkpoints of our model so even if our spot instance is shut down during training, we will have a checkpoint we can go back to once we start training again. Unfortunately, this is only down automatically if you are using Tensorflow, so you need to pass some parameters to make sure you are checkpointing your model during training. Since we are only training IMDB, I'm not too worried about checkpointing, but I've still provided the necessary code to checkpoint our model.\n",
    "\n",
    "Note: while preparing this notebook, it took a few tries for me to get spot instances for training. Even after waiting for 60 minutes, I did not get spot instances and had to run the training again. It may be because I'm asking for 2 instances of quite powerful ml.p3.8xlarge instances. These are not the most powerful, but I expect that they have much less of these GPUs than the more basic ones (the ones with only one GPU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf59f752",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-08-13 16:29:49 Starting - Starting the training job...\n",
      "2021-08-13 16:30:14 Starting - Launching requested ML instancesProfilerReport-1628872183: InProgress\n",
      "......\n",
      "2021-08-13 16:31:14 Starting - Insufficient capacity error from EC2 while launching instances, retrying!..........................................................................................................................................ProfilerReport-1628872183: Stopped\n",
      "....................................................................................................................................................................................................................................\n",
      "2021-08-13 17:32:29 Stopping - Stopping the training job\n",
      "2021-08-13 17:32:29 MaxWaitTimeExceeded - Training job wait time exceeded MaxWaitTimeInSeconds provided\n",
      ".."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Job ended with status 'Stopped' rather than 'Completed'. This could mean the job timed out or stopped early for some other reason: Consider checking whether it completed as you expect.\n"
     ]
    }
   ],
   "source": [
    "# gets role for executing training job\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':'distilbert-base-uncased',\n",
    "    'output_dir':'/opt/ml/checkpoints', # replace with '/opt/ml/checkpoints' when using train.py\n",
    "    'dataset_name': 'imdb', # remove if using custom training script\n",
    "    'do_train': True,\n",
    "    'do_eval': True,\n",
    "    'per_device_train_batch_size': 12,\n",
    "    'num_train_epochs': 5,\n",
    "    'max_seq_length': 128,\n",
    "    'fp16': True,\n",
    "    'pad_to_max_length': True,\n",
    "}\n",
    "\n",
    "# s3 uri where our checkpoints will be uploaded during training\n",
    "job_name = 'using-spot'\n",
    "checkpoint_s3_uri = f's3://{sess.default_bucket()}/{job_name}/checkpoints'\n",
    "\n",
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.6.1'}\n",
    "\n",
    "# configuration for running training on smdistributed Data Parallel\n",
    "# smdistributed = SageMaker Distributed\n",
    "distribution = {'smdistributed': {'dataparallel':{'enabled': True}}}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    entry_point='run_glue.py', # can be replaced with train.py for custom script\n",
    "    source_dir='./examples/pytorch/text-classification', # can be replaced with local scripts directory ('./scripts')\n",
    "    instance_type='ml.p3.8xlarge', # has 4 GPUs\n",
    "    instance_count=2, # changed to 2 instances\n",
    "    base_job_name=job_name,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=True,\n",
    "    max_wait=3600, # This should be equal to or greater than max_run in seconds\n",
    "    max_run=1000, # Expected max run in seconds (so that we don't end up using the instance for too long if there is an issue)\n",
    "    role=role,\n",
    "    git_config=git_config,\n",
    "    transformers_version='4.6.1',\n",
    "    pytorch_version='1.7.1',\n",
    "    py_version='py36',\n",
    "    hyperparameters = hyperparameters\n",
    ")\n",
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit() # put this inside fit if using train.py: {'train': training_input_path, 'test': test_input_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99b10883",
   "metadata": {},
   "source": [
    "## Testing our trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69231501",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch==1.6.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c34305e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tarfile\n",
    "from sagemaker.s3 import S3Downloader\n",
    "\n",
    "local_path = 'imdb_sentiment_distributed_transformer'\n",
    "\n",
    "os.makedirs(local_path, exist_ok=True)\n",
    "\n",
    "# download model S3\n",
    "S3Downloader.download(\n",
    "    s3_uri=huggingface_estimator.model_data, # s3 uri where the trained model is located\n",
    "    local_path=local_path, # local path where *.tar.gz will be saved\n",
    ")\n",
    "\n",
    "# unzip model\n",
    "tar = tarfile.open(f'{local_path}/model.tar.gz', 'r:gz')\n",
    "tar.extractall(path=local_path)\n",
    "tar.close()\n",
    "os.remove(f'{local_path}/model.tar.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72503e7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model=AutoModelForSequenceClassification.from_pretrained(local_path)\n",
    "tokenizer=AutoTokenizer.from_pretrained(local_path)\n",
    "\n",
    "clf = pipeline('text-classification', model=model, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38008fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = 'The Dark Knight is an excellent film!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5485c05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97234de5",
   "metadata": {},
   "source": [
    "## Deploying the endpoint\n",
    "\n",
    "As usual, to deploy our model we simply need to call `.deploy` on our estimator object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56469f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = huggingface_estimator.deploy(1, \"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a16cd5ae",
   "metadata": {},
   "source": [
    "Let's test out our endpoint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6ee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "review = {\"inputs\": \"The Dark Knight is an excellent film!\"}\n",
    "\n",
    "predictor.predict(review)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "355c9df8",
   "metadata": {},
   "source": [
    "## Estimator Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee28e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# container image used for training job\n",
    "print(f\"container image used for training job: \\n{huggingface_estimator.image_uri}\\n\")\n",
    "\n",
    "# s3 uri where the trained model is located\n",
    "print(f\"s3 uri where the trained model is located: \\n{huggingface_estimator.model_data}\\n\")\n",
    "\n",
    "# latest training job name for this estimator\n",
    "print(f\"latest training job name for this estimator: \\n{huggingface_estimator.latest_training_job.name}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b81eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# access the logs of the training job\n",
    "huggingface_estimator.sagemaker_session.logs_for_job(huggingface_estimator.latest_training_job.name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75396666",
   "metadata": {},
   "source": [
    "## Deleting the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e977531",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p36",
   "language": "python",
   "name": "conda_pytorch_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
